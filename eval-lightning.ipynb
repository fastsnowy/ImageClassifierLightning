{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import torch\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'name': 'exp1-oxford-test'}, 'train': {'path': '/home/hrdathaw/mygdev/data/chest_xray/train'}, 'test': {'path': '/home/hrdathaw/mygdev/data/chest_xray/test'}, 'params': {'batch_size': 32, 'num_class': 4, 'epochs': 40, 'm_name': 'vgg16', 'optim_name': 'SGD', 'seed': 42}, 'optim_params': {'lr': 0.01, 'momentum': 0.9}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from hydra import initialize_config_dir, compose\n",
    "with initialize_config_dir(config_dir=f\"/home/hrdathaw/mygdev/src/0429/exp2-xray-gen/config\"):\n",
    "    cfg = compose(config_name='config')\n",
    "    print(cfg)\n",
    "\n",
    "fold_name =cfg.model.name\n",
    "f_path = \"/mnt/d/experiments/7422526/dev/0429/{}\".format(fold_name)\n",
    "os.makedirs(f_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセット・データローダー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = ImageFolder(root=cfg.train.path)\n",
    "y = full.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        # transforms.CenterCrop(224),\n",
    "        # transforms.RandomHorizontalFlip(p=0.1),\n",
    "        # transforms.RandomVerticalFlip(p=0.1),\n",
    "        # transforms.RandomAutocontrast(p=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = ImageFolder(root=cfg.test.path, transform=data_transforms['valid'])\n",
    "test_loader = DataLoader(test_set, cfg.params.batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[self.indices[idx]]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_matrix = torch.zeros(2,2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mymodel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "    batch_size=cfg.params.batch_size, \n",
    "    num_class=cfg.params.num_class, \n",
    "    optim_hparams: dict = None,\n",
    "    m_name = cfg.params.m_name,\n",
    "    optim_name = cfg.params.optim_name,\n",
    "    d_train=None,\n",
    "    d_val=None,\n",
    "    d_test=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_class = num_class\n",
    "        self.m_name = m_name\n",
    "        self.optim_name = optim_name\n",
    "        self.conf_matrix = torchmetrics.ConfusionMatrix(num_class)\n",
    "        # self.d_train = d_train\n",
    "        # self.d_val = d_val\n",
    "        # self.d_test = d_test\n",
    "\n",
    "        if self.m_name == \"vgg16\":\n",
    "            #vgg16\n",
    "            self.model = models.vgg16(pretrained=True)\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad=False\n",
    "            num_feat = self.model.classifier[6].in_features\n",
    "            self.model.classifier[6] = nn.Linear(num_feat, self.hparams.num_class)\n",
    "            print(self.model)\n",
    "            # for param in self.model.features.parameters():\n",
    "            #     param.requires_grad = False\n",
    "            # for param in self.model.avgpool.parameters():\n",
    "            #     param.requires_grad = False\n",
    "        \n",
    "        elif self.m_name == 'resnet18':\n",
    "            # resnet18\n",
    "            self.model = models.resnet18(pretrained=True)\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad=False\n",
    "            num_feat = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(num_feat, self.hparams.num_class)\n",
    "        \n",
    "        else:\n",
    "            assert False, f'不明なmodelです: \"{self.m_name}\"'\n",
    "\n",
    "        # loss\n",
    "        self.loss_module = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = self.loss_module(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self.forward(x)\n",
    "        val_loss = F.cross_entropy(out, y)\n",
    "        out_label = torch.argmax(out, dim=1)\n",
    "        acc = torch.sum(y==out_label) *1.0 /len(y)\n",
    "        self.log('val_loss', val_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'val_loss': val_loss, 'val_acc': acc, 'out': out, 'targets': y}\n",
    "    \n",
    "\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        self.log('avg_loss', avg_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('avg_acc', avg_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'avg_val_loss': avg_loss, 'val_acc': avg_acc}\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self.forward(x)\n",
    "        test_loss = F.cross_entropy(out, y)\n",
    "        out_label = torch.argmax(out, dim=1)\n",
    "        acc = torch.sum(y==out_label) *1.0 /len(y)\n",
    "        self.log('test_loss', test_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('test_acc', acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'test_loss': test_loss, 'test_acc': acc, 'preds':out, 'targets': y}\n",
    "    \n",
    "    def test_epoch_end(self, outputs) -> None:\n",
    "        global sum_matrix\n",
    "        preds = torch.cat([x[\"preds\"] for x in outputs])\n",
    "        targets = torch.cat([x[\"targets\"] for x in outputs])\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
    "        # mat = self.conf_matrix(preds, targets)\n",
    "        # sum_matrix += mat\n",
    "        # names = [\"Bacterialblight\", \"Blast\", \"Brownspot\", \"Tungro\"]\n",
    "        # df = pd.DataFrame(mat.cpu().numpy(), index=names, columns=names)\n",
    "        # plt.figure(figsize = (10,7))\n",
    "        # sns.set(font_scale=1.3)\n",
    "        # # plt.xlabel('Predicted label')\n",
    "        # # plt.ylabel('True label')\n",
    "        # fig_ = sns.heatmap(df, annot=True, cmap='Blues', fmt=\"g\")\n",
    "        # fig_.set(xlabel='Predicited label', ylabel='True label')\n",
    "        # fig_.get_figure()\n",
    "        # # plt.close(fig_)\n",
    "        # wandb.log({\"confusion matrix(testset)\": [wandb.Image(fig_)]})\n",
    "        # df_sum = pd.DataFrame(sum_matrix.cpu().numpy(), index=names, columns=names)\n",
    "        # plt.figure(figsize = (10,7))\n",
    "        # sns.set(font_scale=1.3)\n",
    "        # # plt.xlabel('Predicted label')\n",
    "        # # plt.ylabel('True label')\n",
    "        # fig_s = sns.heatmap(df_sum, annot=True, cmap='Blues', fmt=\"g\")\n",
    "        # fig_s.set(xlabel='Predicited label', ylabel='True label')\n",
    "        # fig_s.get_figure()\n",
    "        # # plt.close(fig_s)\n",
    "        # wandb.log({\"confusion matrix(sum)\": [wandb.Image(fig_s)]})\n",
    "\n",
    "        return {'avg_test_loss': avg_loss, 'test_acc': avg_acc, 'preds': preds, 'targets': targets}\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if self.optim_name == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(self.parameters(), **self.hparams.optim_hparams)\n",
    "        elif self.optim_name == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), **self.hparams.optim_hparams)\n",
    "        else:\n",
    "            assert False, f'不明なOptimizerです: \"{self.hparams.optim_name}\"'\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "pl.seed_everything(cfg.params.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mexpfasts\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hrdathaw/mygdev/src/0429/exp1-oxford-test/wandb/run-20220429_121955-2zuzc7ox</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/expfasts/classifier-imgs/runs/2zuzc7ox\" target=\"_blank\">exp1-oxford-test: fold-0</a></strong> to <a href=\"https://wandb.ai/expfasts/classifier-imgs\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | conf_matrix | ConfusionMatrix  | 0     \n",
      "1 | model       | VGG              | 134 M \n",
      "2 | loss_module | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "16.4 K    Trainable params\n",
      "134 M     Non-trainable params\n",
      "134 M     Total params\n",
      "537.108   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrdathaw/mygdev/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrdathaw/mygdev/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  52%|█████▏    | 85/164 [06:23<05:56,  4.51s/it, loss=0.866, v_num=c7ox, val_loss=0.544, val_acc=0.870, avg_loss=0.543, avg_acc=0.870, train_loss=0.857]   "
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=2022)\n",
    "\n",
    "cv = 0.0\n",
    "cvt = 0.0\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(full)):\n",
    "    net = mymodel(optim_hparams=cfg.optim_params)\n",
    "    name =f\"{fold_name}: fold-{fold}\"\n",
    "    save_path = r\"{}\\fold{}\".format(f_path,fold)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    ckpt = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath=save_path,\n",
    "    filename=\"{epoch}--{val_loss:.3f}\",\n",
    ")\n",
    "#     print(f\"{train_idx}, {val_idx}\")\n",
    "    wandb_logger = WandbLogger(project=cfg.model.project, tags=[\"vgg16\"], name=name)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    d_train = MySubset(full, train_idx, data_transforms['train'])\n",
    "    d_val = MySubset(full, val_idx, data_transforms['valid'])\n",
    "    train_loader = DataLoader(d_train, cfg.params.batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(d_val, cfg.params.batch_size, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=cfg.params.epochs, logger=wandb_logger, callbacks=[early, ckpt])\n",
    "    trainer.fit(model=net, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    cv += trainer.callback_metrics[\"avg_acc\"].mean().item() /kf.n_splits\n",
    "    trainer.test(net, ckpt_path=\"best\", dataloaders=test_loader)\n",
    "    test_acc = trainer.callback_metrics[\"test_acc\"].item()\n",
    "    cvt += test_acc / kf.n_splits\n",
    "    del net\n",
    "    # wandb.finish()\n",
    "    if fold != 4:\n",
    "        wandb.finish()\n",
    "    else:\n",
    "        # wandb.log({\"CV_test\": cvt, \"CV\": cv})\n",
    "        wandb.finish()\n",
    "    print(f\"fold:{fold},test_acc:{test_acc}, cv_test: {cvt}, cv: {cv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3258b8b90c4be647cd2ed3596f969f2beaafd56d342107dc6f600a755f6325ea"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
